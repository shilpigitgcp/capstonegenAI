{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Shilpi Capstone BQ, Langchain & NLP"
      ],
      "metadata": {
        "id": "JwwUIeAnEJDB"
      },
      "id": "JwwUIeAnEJDB"
    },
    {
      "cell_type": "markdown",
      "id": "641ad69c-52a6-4dd3-a9ad-a76540b45bb3",
      "metadata": {
        "id": "641ad69c-52a6-4dd3-a9ad-a76540b45bb3"
      },
      "source": [
        "# Install Vertex AI LLM SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ohPUPez8imvE",
      "metadata": {
        "id": "ohPUPez8imvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f686de65-006f-4d27-e3a9-0e5dd021a071",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1701481303503,
          "user_tz": 360,
          "elapsed": 76770,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33m  WARNING: The script huggingface-cli is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script datasets-cli is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.6/277.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting streamlit\n",
            "  Downloading streamlit-1.29.0-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: importlib-metadata<7,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.8.0)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.23.5)\n",
            "Requirement already satisfied: packaging<24,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.2)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.5.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.6.0)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.5.0)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.2)\n",
            "Collecting validators<1,>=0.2 (from streamlit)\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.2)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit) (3.17.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2023.7.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.12.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Installing collected packages: watchdog, validators, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.40 pydeck-0.8.1b0 smmap-5.0.1 streamlit-1.29.0 validators-0.22.0 watchdog-3.0.0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33m  WARNING: The script transformers-cli is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\n",
        "! pip install --user --quiet google-cloud-aiplatform==1.25.0\n",
        "\n",
        "# Install pandas\n",
        "! pip install --user --quiet pandas\n",
        "\n",
        "# Install HuggingFace Datasets\n",
        "! pip install --user --quiet datasets\n",
        "\n",
        "# Install Python client for Google Search API\n",
        "! pip install --user --quiet google-api-python-client\n",
        "\n",
        "# PDF loader\n",
        "! pip install --user --quiet pypdf\n",
        "\n",
        "# Install similarity search library\n",
        "# TO-DO: Replace with vertex AI Matching Engine\n",
        "! pip install --user --quiet faiss-cpu\n",
        "! pip install streamlit\n",
        "\n",
        "# Install transformers\n",
        "! pip install --user --quiet transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e48a6664-b606-4f8e-8f96-4568f36c13c4",
      "metadata": {
        "id": "e48a6664-b606-4f8e-8f96-4568f36c13c4"
      },
      "source": [
        "# Install LangChain & SQL Alchemy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Vertex AI LLM SDK, langchain and dependencies\n",
        "! pip install --quiet google-cloud-aiplatform langchain==0.0.229 chromadb==0.3.26 pydantic==1.10.8 typing-inspect==0.8.0 typing_extensions==4.5.0 pandas datasets google-api-python-client pypdf faiss-cpu transformers config --upgrade --user"
      ],
      "metadata": {
        "id": "oRXqjNnI6fcN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1701481439283,
          "user_tz": 360,
          "elapsed": 91510,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "c3948ce3-bb7f-4df0-a6ca-5c3ea602c20c"
      },
      "id": "oRXqjNnI6fcN",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m964.4/964.4 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: The script dotenv is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script humanfriendly is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script watchfiles is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script uvicorn is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script langchain is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script coloredlogs is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script onnxruntime_test is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script langchain-server is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d894a722-65fa-4b9c-8200-003d2009e166",
      "metadata": {
        "id": "d894a722-65fa-4b9c-8200-003d2009e166",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1701481481509,
          "user_tz": 360,
          "elapsed": 27691,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "46607ca2-85ca-4f12-8253-d6a0f5eb288a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for SQLAlchemy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.0.229 requires SQLAlchemy<3,>=1.4, but you have sqlalchemy 1.2.0 which is incompatible.\n",
            "bigframes 0.12.0 requires sqlalchemy<3.0dev,>=1.4, but you have sqlalchemy 1.2.0 which is incompatible.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\n",
        "# Below libraries are required to build a SQL engine for BigQuery\n",
        "!pip install --user SQLAlchemy==1.2.0 --quiet\n",
        "!pip install --user sqlalchemy-bigquery --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B-mPnZJdiwkg",
      "metadata": {
        "id": "B-mPnZJdiwkg"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()"
      ],
      "metadata": {
        "id": "yVG_3CNz7tK9",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1701481484898,
          "user_tz": 360,
          "elapsed": 237,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "yVG_3CNz7tK9",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "Tq8CMcvBiywu",
      "metadata": {
        "id": "Tq8CMcvBiywu",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1701481492821,
          "user_tz": 360,
          "elapsed": 172,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"cortex-customer-ss\"  # @param {type:\"string\"} ## Update this with your project id\n",
        "LOCATION = \"us-central1\" # @param {type:\"string\"} ## Continue with us-central1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a91fdaa3-bc11-427c-91df-771acdaaa535",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "a91fdaa3-bc11-427c-91df-771acdaaa535",
        "outputId": "40647508-13a2-42a3-bef0-37dcba99b3bd",
        "executionInfo": {
          "status": "error",
          "timestamp": 1701481495807,
          "user_tz": 360,
          "elapsed": 292,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2ee82bc3ebd2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"LangChain version: {langchain.__version__}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maiplatform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Vertex AI SDK version: {aiplatform.__version__}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import langchain\n",
        "print(f\"LangChain version: {langchain.__version__}\")\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
        "\n",
        "# Initialize Vertex AI SDK\n",
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CZllkRptjWDN",
      "metadata": {
        "id": "CZllkRptjWDN"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import time\n",
        "from typing import Any, Mapping, List, Dict, Optional, Tuple, Union\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "from pydantic import BaseModel, Extra, root_validator\n",
        "\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from langchain.chat_models.base import BaseChatModel\n",
        "from langchain.llms.utils import enforce_stop_tokens\n",
        "from langchain.schema import Generation, LLMResult\n",
        "from langchain.schema import AIMessage, BaseMessage, ChatGeneration, ChatResult, HumanMessage, SystemMessage\n",
        "\n",
        "from vertexai.preview.language_models import TextGenerationResponse, ChatSession\n",
        "\n",
        "\n",
        "def rate_limit(max_per_minute):\n",
        "  period = 60 / max_per_minute\n",
        "  print('Waiting')\n",
        "  while True:\n",
        "    before = time.time()\n",
        "    yield\n",
        "    after = time.time()\n",
        "    elapsed = after - before\n",
        "    sleep_time = max(0, period - elapsed)\n",
        "    if sleep_time > 0:\n",
        "      print('.', end='')\n",
        "      time.sleep(sleep_time)\n",
        "\n",
        "\n",
        "class _VertexCommon(BaseModel):\n",
        "    \"\"\"Wrapper around Vertex AI large language models.\n",
        "\n",
        "    To use, you should have the\n",
        "    ``google.cloud.aiplatform.private_preview.language_models`` python package\n",
        "    installed.\n",
        "    \"\"\"\n",
        "    client: Any = None #: :meta private:\n",
        "    model_name: str = \"text-bison@001\"\n",
        "    \"\"\"Model name to use.\"\"\"\n",
        "\n",
        "    temperature: float = 0.2\n",
        "    \"\"\"What sampling temperature to use.\"\"\"\n",
        "\n",
        "    top_p: int = 0.8\n",
        "    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\n",
        "\n",
        "    top_k: int = 40\n",
        "    \"\"\"The number of highest probability tokens to keep for top-k filtering.\"\"\"\n",
        "\n",
        "    max_output_tokens: int = 200\n",
        "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
        "\n",
        "    @property\n",
        "    def _default_params(self) -> Mapping[str, Any]:\n",
        "        \"\"\"Get the default parameters for calling Vertex AI API.\"\"\"\n",
        "        return {\n",
        "            \"temperature\": self.temperature,\n",
        "            \"top_p\": self.top_p,\n",
        "            \"top_k\": self.top_k,\n",
        "            \"max_output_tokens\": self.max_output_tokens\n",
        "        }\n",
        "\n",
        "    def _predict(self, prompt: str, stop: Optional[List[str]]) -> str:\n",
        "        res = self.client.predict(prompt, **self._default_params)\n",
        "        return self._enforce_stop_words(res.text, stop)\n",
        "\n",
        "    def _enforce_stop_words(self, text: str, stop: Optional[List[str]]) -> str:\n",
        "        if stop:\n",
        "            return enforce_stop_tokens(text, stop)\n",
        "        return text\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of llm.\"\"\"\n",
        "        return \"vertex_ai\"\n",
        "\n",
        "class VertexLLM(_VertexCommon, LLM):\n",
        "    model_name: str = \"text-bison@001\"\n",
        "\n",
        "    @root_validator()\n",
        "    def validate_environment(cls, values: Dict) -> Dict:\n",
        "        \"\"\"Validate that the python package exists in environment.\"\"\"\n",
        "        try:\n",
        "            from vertexai.preview.language_models import TextGenerationModel\n",
        "        except ImportError:\n",
        "            raise ValueError(\n",
        "                \"Could not import Vertex AI LLM python package. \"\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            values[\"client\"] = TextGenerationModel.from_pretrained(values[\"model_name\"])\n",
        "        except AttributeError:\n",
        "            raise ValueError(\n",
        "                \"Could not set Vertex Text Model client.\"\n",
        "            )\n",
        "\n",
        "        return values\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        \"\"\"Call out to Vertex AI's create endpoint.\n",
        "\n",
        "        Args:\n",
        "            prompt: The prompt to pass into the model.\n",
        "\n",
        "        Returns:\n",
        "            The string generated by the model.\n",
        "        \"\"\"\n",
        "        return self._predict(prompt, stop)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class _MessagePair:\n",
        "    \"\"\"InputOutputTextPair represents a pair of input and output texts.\"\"\"\n",
        "\n",
        "    question: HumanMessage\n",
        "    answer: AIMessage\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class _ChatHistory:\n",
        "    \"\"\"InputOutputTextPair represents a pair of input and output texts.\"\"\"\n",
        "\n",
        "    history: List[_MessagePair] = field(default_factory=list)\n",
        "    system_message: Optional[SystemMessage] = None\n",
        "\n",
        "\n",
        "def _parse_chat_history(history: List[BaseMessage]) -> _ChatHistory:\n",
        "    \"\"\"Parses a sequence of messages into history.\n",
        "\n",
        "    A sequency should be either (SystemMessage, HumanMessage, AIMessage,\n",
        "    HumanMessage, AIMessage, ...) or (HumanMessage, AIMessage, HumanMessage,\n",
        "    AIMessage, ...).\n",
        "    \"\"\"\n",
        "    if not history:\n",
        "        return _ChatHistory()\n",
        "    first_message = history[0]\n",
        "    system_message = first_message if isinstance(first_message, SystemMessage) else None\n",
        "    chat_history = _ChatHistory(system_message=system_message)\n",
        "    messages_left = history[1:] if system_message else history\n",
        "    # if len(messages_left) % 2 != 0:\n",
        "    #     raise ValueError(\n",
        "    #         f\"Amount of messages in history should be even, got {len(messages_left)}!\"\n",
        "    #     )\n",
        "    for question, answer in zip(messages_left[::2], messages_left[1::2]):\n",
        "        if not isinstance(question, HumanMessage) or not isinstance(answer, AIMessage):\n",
        "            raise ValueError(\n",
        "                \"A human message should follow a bot one, \"\n",
        "                f\"got {question.type}, {answer.type}.\"\n",
        "            )\n",
        "        chat_history.history.append(_MessagePair(question=question, answer=answer))\n",
        "    return chat_history\n",
        "\n",
        "\n",
        "class _VertexChatCommon(_VertexCommon):\n",
        "    \"\"\"Wrapper around Vertex AI Chat large language models.\n",
        "\n",
        "    To use, you should have the\n",
        "    ``vertexai.preview.language_models`` python package\n",
        "    installed.\n",
        "    \"\"\"\n",
        "    model_name: str = \"chat-bison@001\"\n",
        "    \"\"\"Model name to use.\"\"\"\n",
        "\n",
        "    @root_validator()\n",
        "    def validate_environment(cls, values: Dict) -> Dict:\n",
        "        \"\"\"Validate that the python package exists in environment.\"\"\"\n",
        "        try:\n",
        "            from vertexai.preview.language_models import ChatModel\n",
        "        except ImportError:\n",
        "            raise ValueError(\n",
        "                \"Could not import Vertex AI LLM python package. \"\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            values[\"client\"] = ChatModel.from_pretrained(values[\"model_name\"])\n",
        "        except AttributeError:\n",
        "            raise ValueError(\n",
        "                \"Could not set Vertex Text Model client.\"\n",
        "            )\n",
        "\n",
        "        return values\n",
        "\n",
        "    def _response_to_chat_results(\n",
        "        self, response: TextGenerationResponse, stop: Optional[List[str]]\n",
        "    ) -> ChatResult:\n",
        "        text = self._enforce_stop_words(response.text, stop)\n",
        "        return ChatResult(generations=[ChatGeneration(message=AIMessage(content=text))])\n",
        "\n",
        "\n",
        "class VertexChat(_VertexChatCommon, BaseChatModel):\n",
        "    \"\"\"Wrapper around Vertex AI large language models.\n",
        "\n",
        "    To use, you should have the\n",
        "    ``vertexai.preview.language_models`` python package\n",
        "    installed.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name: str = \"chat-bison@001\"\n",
        "    chat: Any = None  #: :meta private:\n",
        "\n",
        "    def send_message(\n",
        "        self, message: Union[HumanMessage, str], stop: Optional[List[str]] = None\n",
        "    ) -> ChatResult:\n",
        "        text = message.content if isinstance(message, BaseMessage) else message\n",
        "        response = self.chat.send_message(text)\n",
        "        text = self._enforce_stop_words(response.text, stop)\n",
        "        return ChatResult(generations=[ChatGeneration(message=AIMessage(content=text))])\n",
        "\n",
        "    def _generate(\n",
        "        self, messages: List[BaseMessage], stop: Optional[List[str]] = None\n",
        "    ) -> ChatResult:\n",
        "        if not messages:\n",
        "            raise ValueError(\n",
        "                \"You should provide at least one message to start the chat!\"\n",
        "            )\n",
        "        question = messages[-1]\n",
        "        if not isinstance(question, HumanMessage):\n",
        "            raise ValueError(\n",
        "                f\"Last message in the list should be from human, got {question.type}.\"\n",
        "            )\n",
        "        self.start_chat(messages[:-1])\n",
        "        return self.send_message(question)\n",
        "\n",
        "    def start_chat(self, messages: List[BaseMessage]) -> None:\n",
        "        \"\"\"Starts a chat.\"\"\"\n",
        "        history = _parse_chat_history(messages)\n",
        "        context = history.system_message.content if history.system_message else None\n",
        "        self.chat = self.client.start_chat(context=context, **self._default_params)\n",
        "        for pair in history.history:\n",
        "            self.chat._history.append((pair.question.content, pair.answer.content))\n",
        "\n",
        "    def clear_chat(self) -> None:\n",
        "        self.chat = None\n",
        "\n",
        "    @property\n",
        "    def history(self) -> List[BaseMessage]:\n",
        "        \"\"\"Chat history.\"\"\"\n",
        "        history: List[BaseMessage] = []\n",
        "        if self.chat:\n",
        "            for question, answer in self.chat._history:\n",
        "                history.append(HumanMessage(content=question))\n",
        "                history.append(AIMessage(content=answer))\n",
        "        return history\n",
        "\n",
        "    async def _agenerate(\n",
        "        self, messages: List[BaseMessage], stop: Optional[List[str]] = None\n",
        "    ) -> ChatResult:\n",
        "        raise NotImplementedError(\n",
        "            \"\"\"Vertex AI doesn't support async requests at the moment.\"\"\"\n",
        "        )\n",
        "\n",
        "class VertexMultiTurnChat(_VertexChatCommon, BaseChatModel):\n",
        "    \"\"\"Wrapper around Vertex AI large language models.\"\"\"\n",
        "\n",
        "    model_name: str = \"chat-bison@001\"\n",
        "    chat: Optional[ChatSession] = None\n",
        "\n",
        "    def clear_chat(self) -> None:\n",
        "        self.chat = None\n",
        "\n",
        "    def start_chat(self, message: Optional[SystemMessage] = None) -> None:\n",
        "        if self.chat:\n",
        "            raise ValueError(\"Chat has already been started. Please, clear it first.\")\n",
        "        if message and not isinstance(message, SystemMessage):\n",
        "            raise ValueError(\"Context should be a system message\")\n",
        "        context = message.content if message else None\n",
        "        self.chat = self.client.start_chat(context=context, **self._default_params)\n",
        "\n",
        "    @property\n",
        "    def history(self) -> List[Tuple[str]]:\n",
        "        \"\"\"Chat history.\"\"\"\n",
        "        if self.chat:\n",
        "            return self.chat._history\n",
        "        return []\n",
        "\n",
        "    def _generate(\n",
        "        self, messages: List[BaseMessage], stop: Optional[List[str]] = None\n",
        "    ) -> ChatResult:\n",
        "        if len(messages) != 1:\n",
        "            raise ValueError(\n",
        "                \"You should send exactly one message to the chat each turn.\"\n",
        "            )\n",
        "        if not self.chat:\n",
        "            raise ValueError(\"You should start_chat first!\")\n",
        "        response = self.chat.send_message(messages[0].content)\n",
        "        return self._response_to_chat_results(response, stop=stop)\n",
        "\n",
        "    async def _agenerate(\n",
        "        self, messages: List[BaseMessage], stop: Optional[List[str]] = None\n",
        "    ) -> ChatResult:\n",
        "        raise NotImplementedError(\n",
        "            \"\"\"Vertex AI doesn't support async requests at the moment.\"\"\"\n",
        "        )\n",
        "\n",
        "class VertexEmbeddings(Embeddings, BaseModel):\n",
        "    \"\"\"Wrapper around Vertex AI large language models embeddings API.\n",
        "\n",
        "    To use, you should have the\n",
        "    ``google.cloud.aiplatform.private_preview.language_models`` python package\n",
        "    installed.\n",
        "    \"\"\"\n",
        "    model_name: str = \"textembedding-gecko@001\"\n",
        "    \"\"\"Model name to use.\"\"\"\n",
        "\n",
        "    model: Any\n",
        "    requests_per_minute: int = 15\n",
        "\n",
        "\n",
        "    @root_validator()\n",
        "    def validate_environment(cls, values: Dict) -> Dict:\n",
        "        \"\"\"Validate that the python package exists in environment.\"\"\"\n",
        "        try:\n",
        "            from vertexai.preview.language_models import TextEmbeddingModel\n",
        "\n",
        "        except ImportError:\n",
        "            raise ValueError(\n",
        "                \"Could not import Vertex AI LLM python package. \"\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            values[\"model\"] = TextEmbeddingModel\n",
        "\n",
        "        except AttributeError:\n",
        "            raise ValueError(\n",
        "                \"Could not set Vertex Text Model client.\"\n",
        "            )\n",
        "\n",
        "        return values\n",
        "\n",
        "    class Config:\n",
        "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
        "\n",
        "        extra = Extra.forbid\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "      \"\"\"Call Vertex LLM embedding endpoint for embedding docs\n",
        "      Args:\n",
        "          texts: The list of texts to embed.\n",
        "      Returns:\n",
        "          List of embeddings, one for each text.\n",
        "      \"\"\"\n",
        "      self.model = self.model.from_pretrained(self.model_name)\n",
        "\n",
        "      limiter = rate_limit(self.requests_per_minute)\n",
        "      results = []\n",
        "      docs = list(texts)\n",
        "\n",
        "      while docs:\n",
        "        # Working in batches of 2 because the API apparently won't let\n",
        "        # us send more than 2 documents per request to get embeddings.\n",
        "        head, docs = docs[:2], docs[2:]\n",
        "        # print(f'Sending embedding request for: {head!r}')\n",
        "        chunk = self.model.get_embeddings(head)\n",
        "        results.extend(chunk)\n",
        "        next(limiter)\n",
        "\n",
        "      return [r.values for r in results]\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "      \"\"\"Call Vertex LLM embedding endpoint for embedding query text.\n",
        "      Args:\n",
        "        text: The text to embed.\n",
        "      Returns:\n",
        "        Embedding for the text.\n",
        "      \"\"\"\n",
        "      single_result = self.embed_documents([text])\n",
        "      return single_result[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c344bf2-d36b-4fe8-8431-afaecc01d5aa",
      "metadata": {
        "id": "1c344bf2-d36b-4fe8-8431-afaecc01d5aa"
      },
      "source": [
        "# Create your input DataSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eVpPcvsrkzCk",
      "metadata": {
        "id": "eVpPcvsrkzCk"
      },
      "outputs": [],
      "source": [
        "REQUESTS_PER_MINUTE = 100\n",
        "\n",
        "llm = VertexLLM(\n",
        "    model_name='text-bison@001',\n",
        "    max_output_tokens=1024,\n",
        "    temperature=0.1,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "chat = VertexChat()\n",
        "\n",
        "mchat = VertexMultiTurnChat(max_output_tokens=1024)\n",
        "\n",
        "embedding = VertexEmbeddings(requests_per_minute=REQUESTS_PER_MINUTE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "112822f8-04b9-4d66-8272-ab86f08fca44",
      "metadata": {
        "id": "112822f8-04b9-4d66-8272-ab86f08fca44"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "from google.cloud.bigquery import Client\n",
        "client = Client(project=PROJECT_ID)\n",
        "dataset_id = \"SAP_REPORTING\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2260b9dd-6fc1-4fa0-9f67-79682ecf5546",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2260b9dd-6fc1-4fa0-9f67-79682ecf5546",
        "outputId": "d4c11526-d9f7-4d5a-9829-8f238f99ee62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<google.cloud.bigquery.table._EmptyRowIterator object at 0x7bea9c746200>\n"
          ]
        }
      ],
      "source": [
        "query = \"\"\"\n",
        "CREATE SCHEMA `{PROJECT_ID}.{dataset_id}`\n",
        "OPTIONS(\n",
        "  location=\"us\"\n",
        "  )\n",
        "\"\"\".format(\n",
        "    PROJECT_ID=PROJECT_ID, dataset_id=dataset_id\n",
        ")\n",
        "query_job = client.query(query)\n",
        "print(query_job.result())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fe18f28-5c86-409a-820c-9453b5739eae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fe18f28-5c86-409a-820c-9453b5739eae",
        "outputId": "83b492c7-a677-4081-bc15-4ea0ddd2c968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<google.cloud.bigquery.table._EmptyRowIterator object at 0x7bea9c73c490>\n"
          ]
        }
      ],
      "source": [
        "query = \"\"\"\n",
        "create or replace table `{PROJECT_ID}.{dataset_id}.sales`\n",
        "as\n",
        "select invoice_and_item_number,\n",
        "date,\n",
        "store_number,\n",
        "store_name,\n",
        "address,\n",
        "city,\n",
        "zip_code,\n",
        "county_number,\n",
        "county,\n",
        "category,\n",
        "category_name,\n",
        "vendor_number,\n",
        "vendor_name,\n",
        "item_number,\n",
        "item_description,\n",
        "pack,\n",
        "bottle_volume_ml,\n",
        "state_bottle_cost,\n",
        "state_bottle_retail,\n",
        "bottles_sold,\n",
        "sale_dollars,\n",
        "volume_sold_liters,\n",
        "volume_sold_gallons\n",
        "from `bigquery-public-data.iowa_liquor_sales.sales`\n",
        "\"\"\".format(\n",
        "    PROJECT_ID=PROJECT_ID, dataset_id=dataset_id\n",
        ")\n",
        "query_job = client.query(query)\n",
        "print(query_job.result())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e6882df-3185-4484-841b-d4fcf1bd3529",
      "metadata": {
        "id": "9e6882df-3185-4484-841b-d4fcf1bd3529"
      },
      "outputs": [],
      "source": [
        "# @title Specify Project details and location of the BQ table\n",
        "\n",
        "project_id = PROJECT_ID  # @param {type:\"string\"}\n",
        "location = LOCATION  # @param {type:\"string\"}\n",
        "dataset_id = 'SAP_DATA' # @param {type:\"string\"}\n",
        "table_name = 'vbak' # @param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97cccf79-dc0d-4df6-a1c5-0f19c042dc14",
      "metadata": {
        "id": "97cccf79-dc0d-4df6-a1c5-0f19c042dc14"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import *\n",
        "from sqlalchemy.engine import create_engine\n",
        "from sqlalchemy.schema import *\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a70ec4f-abb2-4abb-b2cc-eb3ffa70476e",
      "metadata": {
        "id": "2a70ec4f-abb2-4abb-b2cc-eb3ffa70476e"
      },
      "outputs": [],
      "source": [
        "table_uri = f\"bigquery://{project_id}/{dataset_id}\"\n",
        "engine = create_engine(f\"bigquery://{project_id}/{dataset_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4d0daf0-e982-4e0f-89f3-b72416ea74a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4d0daf0-e982-4e0f-89f3-b72416ea74a2",
        "outputId": "dc31802e-a3f1-4410-ffc1-9bd89565fa39",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1694574460832,
          "user_tz": 300,
          "elapsed": 3992,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('250', '0060000000', datetime.date(2022, 1, 19), datetime.time(10, 10, 42), 'MALVIYAH', None, None, datetime.date(2022, 1, 19), 'H', '0', 'RE', '101', None, None, None, None, Decimal('97.000000000'), 'USD', 'US10', '10', '02', None, None, None, None, None, None, '0000000020', datetime.date(2022, 1, 19), '1', None, None, None, 'ZUS001', '01', 'RE', '100', None, None, None, None, None, None, None, None, None, None, '0000000002', None, None, 'USD', datetime.date(2022, 1, 19), None, None, None, None, None, None, 'CYMB', None, None, None, None, None, None, None, None, None, None, None, None, 'A', None, None, '0090000006', None, 'USA1', '1', '1', '1', None, None, None, None, None, None, 'PO1', '0090000006', 'M', None, None, None, None, None, None, None, None, None, None, None, None, None, 'US', None, None, None, datetime.date(2022, 1, 19), None, 'GW4AW00F7joUeW7fJ2K5Am', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 'L', False, datetime.datetime(2022, 3, 26, 2, 40, 14, 167739, tzinfo=<UTC>))"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "query=f\"\"\"SELECT * FROM `{project_id}.{dataset_id}.{table_name}`\"\"\"\n",
        "engine.execute(query).first()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import SQLDatabase, SQLDatabaseChain\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "def bq_qna(question):\n",
        "  #create SQLDatabase instance from BQ engine\n",
        "  db = SQLDatabase(engine=engine,metadata=MetaData(bind=engine),include_tables=[table_name])\n",
        "\n",
        "\n",
        "  # db = SQLDatabase(engine=engine,metadata=MetaData(bind=engine),include_tables=['bigquery-public-data.iowa_liquor_sales.sales'])\n",
        "\n",
        "  #create SQL DB Chain with the initialized LLM and above SQLDB instance\n",
        "  db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True, return_intermediate_steps=True)\n",
        "\n",
        "  #Define prompt for BigQuery SQL\n",
        "  _googlesql_prompt = \"\"\"You are a BigQuery SQL expert. Given an input question, first create a syntactically correct BigQuery query to run, then look at the results of the query and return the answer to the input question.\n",
        "  Unless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per BigQuery SQL. You can order the results to return the most informative data in the database.\n",
        "  Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in backticks (`) to denote them as delimited identifiers.\n",
        "  Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table. Add additional information about the relevant findings from the query\n",
        "  Use the following format:\n",
        "  Question: \"Question here\"\n",
        "  SQLQuery: \"SQL Query to run\"\n",
        "  SQLResult: \"Result of the SQLQuery\"\n",
        "  Answer: \"Final answer here\"\n",
        "  Only use the following tables:\n",
        "  {table_info}\n",
        "\n",
        "  If someone asks for specific month, use ActivityDate between current month's start date and current month's end date\n",
        "\n",
        "  Question: {input}\"\"\"\n",
        "\n",
        "  BigQuerySQL_PROMPT = PromptTemplate(\n",
        "      input_variables=[\"input\", \"table_info\", \"top_k\"],\n",
        "      template=_googlesql_prompt,\n",
        "  )\n",
        "\n",
        "  #passing question to the prompt template\n",
        "  final_prompt = BigQuerySQL_PROMPT.format(input=question, table_info=table_name, top_k=10000)\n",
        "\n",
        "  #pass final prompt to SQL Chain\n",
        "  output = db_chain(final_prompt)\n",
        "\n",
        "\n",
        "  return output['result'], output['intermediate_steps'][0]\n"
      ],
      "metadata": {
        "id": "CrFpnxmlanFC"
      },
      "id": "CrFpnxmlanFC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "946de585-312b-43d2-9670-e3d6e8498951",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "946de585-312b-43d2-9670-e3d6e8498951",
        "outputId": "0b1911b8-c9a4-473f-e686-f463a141c51a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1694574474899,
          "user_tz": 300,
          "elapsed": 120,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vbak\n"
          ]
        }
      ],
      "source": [
        "print(table_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1bc1a71-1aeb-424f-ae54-1f1c61a4a605",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1bc1a71-1aeb-424f-ae54-1f1c61a4a605",
        "outputId": "dc3bfee6-3f74-4f1f-de0d-09a5674b4a77",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1694574564308,
          "user_tz": 300,
          "elapsed": 7397,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "You are a BigQuery SQL expert. Given an input question, first create a syntactically correct BigQuery query to run, then look at the results of the query and return the answer to the input question.\n",
            "  Unless the user specifies in the question a specific number of examples to obtain, query for at most 10000 results using the LIMIT clause as per BigQuery SQL. You can order the results to return the most informative data in the database.\n",
            "  Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in backticks (`) to denote them as delimited identifiers.\n",
            "  Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table. Add additional information about the relevant findings from the query\n",
            "  Use the following format:\n",
            "  Question: \"Question here\"\n",
            "  SQLQuery: \"SQL Query to run\"\n",
            "  SQLResult: \"Result of the SQLQuery\"\n",
            "  Answer: \"Final answer here\"\n",
            "  Only use the following tables:\n",
            "  vbak\n",
            "\n",
            "  If someone asks for specific month, use ActivityDate between current month's start date and current month's end date\n",
            "\n",
            "  Question: what are my top 5 product names by sales volume\n",
            "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT auart, SUM(netwr) AS total_sales FROM vbak GROUP BY auart ORDER BY total_sales DESC LIMIT 5\u001b[0m\n",
            "SQLResult: \u001b[33;1m\u001b[1;3m[('TA', Decimal('6214252245.390000000')), ('ZPSI', Decimal('4173004000.000000000')), ('ZPSQ', Decimal('3477503000.000000000')), ('ZPSO', Decimal('3477501000.000000000')), ('RFRC', Decimal('553046130.000000000'))]\u001b[0m\n",
            "Answer:\u001b[32;1m\u001b[1;3mThe top 5 product names by sales volume are TA, ZPSI, ZPSO, ZPSQ, and RFRC.\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('The top 5 product names by sales volume are TA, ZPSI, ZPSO, ZPSQ, and RFRC.',\n",
              " {'input': 'You are a BigQuery SQL expert. Given an input question, first create a syntactically correct BigQuery query to run, then look at the results of the query and return the answer to the input question.\\n  Unless the user specifies in the question a specific number of examples to obtain, query for at most 10000 results using the LIMIT clause as per BigQuery SQL. You can order the results to return the most informative data in the database.\\n  Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in backticks (`) to denote them as delimited identifiers.\\n  Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table. Add additional information about the relevant findings from the query\\n  Use the following format:\\n  Question: \"Question here\"\\n  SQLQuery: \"SQL Query to run\"\\n  SQLResult: \"Result of the SQLQuery\"\\n  Answer: \"Final answer here\"\\n  Only use the following tables:\\n  vbak\\n\\n  If someone asks for specific month, use ActivityDate between current month\\'s start date and current month\\'s end date\\n\\n  Question: what are my top 5 product names by sales volume\\nSQLQuery:SELECT auart, SUM(netwr) AS total_sales FROM vbak GROUP BY auart ORDER BY total_sales DESC LIMIT 5\\nSQLResult: [(\\'TA\\', Decimal(\\'6214252245.390000000\\')), (\\'ZPSI\\', Decimal(\\'4173004000.000000000\\')), (\\'ZPSQ\\', Decimal(\\'3477503000.000000000\\')), (\\'ZPSO\\', Decimal(\\'3477501000.000000000\\')), (\\'RFRC\\', Decimal(\\'553046130.000000000\\'))]\\nAnswer:',\n",
              "  'top_k': '5',\n",
              "  'dialect': 'bigquery',\n",
              "  'table_info': '\\nCREATE TABLE `vbak` (\\n\\t`mandt` STRING, \\n\\t`vbeln` STRING, \\n\\t`erdat` DATE, \\n\\t`erzet` TIME, \\n\\t`ernam` STRING, \\n\\t`angdt` DATE, \\n\\t`bnddt` DATE, \\n\\t`audat` DATE, \\n\\t`vbtyp` STRING, \\n\\t`trvog` STRING, \\n\\t`auart` STRING, \\n\\t`augru` STRING, \\n\\t`gwldt` DATE, \\n\\t`submi` STRING, \\n\\t`lifsk` STRING, \\n\\t`faksk` STRING, \\n\\t`netwr` NUMERIC, \\n\\t`waerk` STRING, \\n\\t`vkorg` STRING, \\n\\t`vtweg` STRING, \\n\\t`spart` STRING, \\n\\t`vkgrp` STRING, \\n\\t`vkbur` STRING, \\n\\t`gsber` STRING, \\n\\t`gskst` STRING, \\n\\t`guebg` DATE, \\n\\t`gueen` DATE, \\n\\t`knumv` STRING, \\n\\t`vdatu` DATE, \\n\\t`vprgr` STRING, \\n\\t`autlf` STRING, \\n\\t`vbkla` STRING, \\n\\t`vbklt` STRING, \\n\\t`kalsm` STRING, \\n\\t`vsbed` STRING, \\n\\t`fkara` STRING, \\n\\t`awahr` STRING, \\n\\t`ktext` STRING, \\n\\t`bstnk` STRING, \\n\\t`bsark` STRING, \\n\\t`bstdk` DATE, \\n\\t`bstzd` STRING, \\n\\t`ihrez` STRING, \\n\\t`bname` STRING, \\n\\t`telf1` STRING, \\n\\t`mahza` NUMERIC, \\n\\t`mahdt` DATE, \\n\\t`kunnr` STRING, \\n\\t`kostl` STRING, \\n\\t`stafo` STRING, \\n\\t`stwae` STRING, \\n\\t`aedat` DATE, \\n\\t`kvgr1` STRING, \\n\\t`kvgr2` STRING, \\n\\t`kvgr3` STRING, \\n\\t`kvgr4` STRING, \\n\\t`kvgr5` STRING, \\n\\t`knuma` STRING, \\n\\t`kokrs` STRING, \\n\\t`ps_psp_pnr` STRING, \\n\\t`kurst` STRING, \\n\\t`kkber` STRING, \\n\\t`knkli` STRING, \\n\\t`grupp` STRING, \\n\\t`sbgrp` STRING, \\n\\t`ctlpc` STRING, \\n\\t`cmwae` STRING, \\n\\t`cmfre` DATE, \\n\\t`cmnup` DATE, \\n\\t`cmngv` DATE, \\n\\t`amtbl` NUMERIC, \\n\\t`hityp_pr` STRING, \\n\\t`abrvw` STRING, \\n\\t`abdis` STRING, \\n\\t`vgbel` STRING, \\n\\t`objnr` STRING, \\n\\t`bukrs_vf` STRING, \\n\\t`taxk1` STRING, \\n\\t`taxk2` STRING, \\n\\t`taxk3` STRING, \\n\\t`taxk4` STRING, \\n\\t`taxk5` STRING, \\n\\t`taxk6` STRING, \\n\\t`taxk7` STRING, \\n\\t`taxk8` STRING, \\n\\t`taxk9` STRING, \\n\\t`xblnr` STRING, \\n\\t`zuonr` STRING, \\n\\t`vgtyp` STRING, \\n\\t`kalsm_ch` STRING, \\n\\t`agrzr` STRING, \\n\\t`aufnr` STRING, \\n\\t`qmnum` STRING, \\n\\t`vbeln_grp` STRING, \\n\\t`scheme_grp` STRING, \\n\\t`abruf_part` STRING, \\n\\t`abhod` DATE, \\n\\t`abhov` TIME, \\n\\t`abhob` TIME, \\n\\t`rplnr` STRING, \\n\\t`vzeit` TIME, \\n\\t`stceg_l` STRING, \\n\\t`landtx` STRING, \\n\\t`xegdr` STRING, \\n\\t`enqueue_grp` STRING, \\n\\t`dat_fzau` DATE, \\n\\t`fmbdat` DATE, \\n\\t`vsnmr_v` STRING, \\n\\t`handle` STRING, \\n\\t`proli` STRING, \\n\\t`cont_dg` STRING, \\n\\t`crm_guid` STRING, \\n\\t`upd_tmstmp` NUMERIC, \\n\\t`msr_id` STRING, \\n\\t`tm_ctrl_key` STRING, \\n\\t`handoverloc` STRING, \\n\\t`dataaging` DATE, \\n\\t`glo_log_ref1_hd` STRING, \\n\\t`psm_budat` DATE, \\n\\t`fsh_kvgr6` STRING, \\n\\t`fsh_kvgr7` STRING, \\n\\t`fsh_kvgr8` STRING, \\n\\t`fsh_kvgr9` STRING, \\n\\t`fsh_kvgr10` STRING, \\n\\t`fsh_rereg` STRING, \\n\\t`fsh_cq_check` STRING, \\n\\t`fsh_vrsn_status` STRING, \\n\\t`fsh_transaction` STRING, \\n\\t`fsh_vas_cg` STRING, \\n\\t`fsh_candate` DATE, \\n\\t`fsh_ss` STRING, \\n\\t`fsh_os_stg_change` STRING, \\n\\t`swenr` STRING, \\n\\t`smenr` STRING, \\n\\t`phase` STRING, \\n\\t`mtlaur` STRING, \\n\\t`stage` INT64, \\n\\t`hb_cont_reason` STRING, \\n\\t`hb_expdate` DATE, \\n\\t`hb_resdate` DATE, \\n\\t`mill_appl_id` STRING, \\n\\t`tas` STRING, \\n\\t`betc` STRING, \\n\\t`mod_allow` STRING, \\n\\t`cancel_allow` STRING, \\n\\t`pay_method` STRING, \\n\\t`bpn` STRING, \\n\\t`rep_freq` STRING, \\n\\t`logsysb` STRING, \\n\\t`kalcd` STRING, \\n\\t`multi` STRING, \\n\\t`sppaym` STRING, \\n\\t`wtysc_clm_hdr` STRING, \\n\\t`operation_flag` STRING, \\n\\t`is_deleted` BOOL, \\n\\t`recordstamp` TIMESTAMP\\n)\\n\\n/*\\n3 rows from vbak table:\\nmandt\\tvbeln\\terdat\\terzet\\ternam\\tangdt\\tbnddt\\taudat\\tvbtyp\\ttrvog\\tauart\\taugru\\tgwldt\\tsubmi\\tlifsk\\tfaksk\\tnetwr\\twaerk\\tvkorg\\tvtweg\\tspart\\tvkgrp\\tvkbur\\tgsber\\tgskst\\tguebg\\tgueen\\tknumv\\tvdatu\\tvprgr\\tautlf\\tvbkla\\tvbklt\\tkalsm\\tvsbed\\tfkara\\tawahr\\tktext\\tbstnk\\tbsark\\tbstdk\\tbstzd\\tihrez\\tbname\\ttelf1\\tmahza\\tmahdt\\tkunnr\\tkostl\\tstafo\\tstwae\\taedat\\tkvgr1\\tkvgr2\\tkvgr3\\tkvgr4\\tkvgr5\\tknuma\\tkokrs\\tps_psp_pnr\\tkurst\\tkkber\\tknkli\\tgrupp\\tsbgrp\\tctlpc\\tcmwae\\tcmfre\\tcmnup\\tcmngv\\tamtbl\\thityp_pr\\tabrvw\\tabdis\\tvgbel\\tobjnr\\tbukrs_vf\\ttaxk1\\ttaxk2\\ttaxk3\\ttaxk4\\ttaxk5\\ttaxk6\\ttaxk7\\ttaxk8\\ttaxk9\\txblnr\\tzuonr\\tvgtyp\\tkalsm_ch\\tagrzr\\taufnr\\tqmnum\\tvbeln_grp\\tscheme_grp\\tabruf_part\\tabhod\\tabhov\\tabhob\\trplnr\\tvzeit\\tstceg_l\\tlandtx\\txegdr\\tenqueue_grp\\tdat_fzau\\tfmbdat\\tvsnmr_v\\thandle\\tproli\\tcont_dg\\tcrm_guid\\tupd_tmstmp\\tmsr_id\\ttm_ctrl_key\\thandoverloc\\tdataaging\\tglo_log_ref1_hd\\tpsm_budat\\tfsh_kvgr6\\tfsh_kvgr7\\tfsh_kvgr8\\tfsh_kvgr9\\tfsh_kvgr10\\tfsh_rereg\\tfsh_cq_check\\tfsh_vrsn_status\\tfsh_transaction\\tfsh_vas_cg\\tfsh_candate\\tfsh_ss\\tfsh_os_stg_change\\tswenr\\tsmenr\\tphase\\tmtlaur\\tstage\\thb_cont_reason\\thb_expdate\\thb_resdate\\tmill_appl_id\\ttas\\tbetc\\tmod_allow\\tcancel_allow\\tpay_method\\tbpn\\trep_freq\\tlogsysb\\tkalcd\\tmulti\\tsppaym\\twtysc_clm_hdr\\toperation_flag\\tis_deleted\\trecordstamp\\n250\\t0060000000\\t2022-01-19\\t10:10:42\\tMALVIYAH\\tNone\\tNone\\t2022-01-19\\tH\\t0\\tRE\\t101\\tNone\\tNone\\tNone\\tNone\\t97.000000000\\tUSD\\tUS10\\t10\\t02\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\t0000000020\\t2022-01-19\\t1\\tNone\\tNone\\tNone\\tZUS001\\t01\\tRE\\t100\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\t0000000002\\tNone\\tNone\\tUSD\\t2022-01-19\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tCYMB\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tA\\tNone\\tNone\\t0090000006\\tNone\\tUSA1\\t1\\t1\\t1\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tPO1\\t0090000006\\tM\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tUS\\tNone\\tNone\\tNone\\t2022-01-19\\tNone\\tGW4AW00F7joUeW7fJ2K5Am\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tL\\tFalse\\t2022-03-26 02:40:14.167739+00:00\\n250\\t0060000003\\t2022-02-07\\t05:40:21\\tMALVIYAH\\tNone\\tNone\\t2022-02-06\\tH\\t0\\tRE\\t102\\tNone\\tNone\\tNone\\tNone\\t97.000000000\\tUSD\\tUS10\\t10\\t03\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\t0000000086\\t2022-02-07\\t1\\tNone\\tNone\\tNone\\tZUS001\\t01\\tRE\\t100\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\t0000000002\\tNone\\tNone\\tUSD\\t2022-02-07\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tCYMB\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tA\\tNone\\tNone\\t0090000041\\tNone\\tUSA1\\t1\\t1\\t1\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tPO\\t0090000041\\tM\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tUS\\tNone\\tNone\\tNone\\t2022-02-07\\tNone\\tGW4AW00F7joX{muUjSI5Am\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tL\\tFalse\\t2022-03-26 02:40:14.167739+00:00\\n250\\t0060000005\\t2022-02-23\\t18:16:50\\tSIBASISM\\tNone\\tNone\\t2022-02-23\\tH\\t0\\tRE\\t101\\tNone\\tNone\\tNone\\tNone\\t19.400000000\\tUSD\\tUS10\\t10\\t02\\tNone\\tUS21\\tNone\\tNone\\tNone\\tNone\\t0000000103\\t2022-02-23\\t1\\tNone\\tNone\\tNone\\tZUS001\\t01\\tRE\\t100\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\t0000000002\\tNone\\tNone\\tUSD\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tCYMB\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tA\\tNone\\tNone\\t0090000051\\tNone\\tUSA1\\t1\\t1\\t1\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tPO10\\t0090000051\\tM\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tUS\\tNone\\tNone\\tNone\\t2022-02-23\\tNone\\tGW4AW00F7jobcfYVBvY5Am\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tNone\\tL\\tFalse\\t2022-03-26 02:40:14.167739+00:00\\n*/',\n",
              "  'stop': ['\\nSQLResult:']})"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "bq_qna('what are my top 5 product names by sales volume')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "environment": {
      "kernel": "python3",
      "name": "tf2-gpu.2-11.m108",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}